---
title: "Assignment 2 submission"
author: "Anya Wongreantong, Willem Barnard, Dhvani Vyas"
date: "2024-10-16"
output:
  pdf_document: default
  html_document: default
---
Perform an exploratory analysis of the measured features and see if we can design machine learning tools to separate the benign (B) and malignant (M) cells.
```{r}
#load the data

ovarian.dataset <- read.delim("ovarian.data", sep=",", header = FALSE)
features <- c("perimeter", "area", "smoothness", "symmetry", "concavity", paste("protein", seq(1, 25), sep=""))
names(ovarian.dataset) <- c("cell_id", "diagnosis", features) 

head(ovarian.dataset)
```
## Question 1 dimensionality reduction

Q1.1. Perform PCA on the features of the dataset. How much of the variation in the data is associated with PC1?

```{r}
ovarian.pca <- prcomp(ovarian.dataset[features], center = TRUE, scale. = TRUE)
summary(ovarian.pca)$importance[2,1]
```

Q1.2. You want to represent 90% of the variance in the data by dimensionality reduction. How many PCs do you need to achieve this? In other word, what would be the dimensionality of the reduced feature space so that you preserve 90% of the variability in the data? 

```{r}
variance <- summary(ovarian.pca)$importance[2,]
which(cumsum(variance) >= 0.90)[1]
```

Q1.3. As you should know by now, PCA transforms the data into a new space. In a 2-D plot, can you plot the observations corresponding to the first two important PCs? Note, use two different colors to represent the two classes of cells.

```{r}
# install.packages("devtools")
library(devtools)
# install.packages("remotes")
remotes::install_github("vqv/ggbiplot")
library(ggbiplot)

ggbiplot(ovarian.pca, choices = c(1,2), obs.scale = 1, var.scale = 1,
         groups = ovarian.dataset$diagnosis, ellipse = TRUE, circle = TRUE) + 
  scale_color_manual(values = c("royalblue", "plum")) 
```

Q1.4. Can you plot the "area" and "concavity"  features associated with the cells?
```{r}
library(ggplot2)

ggplot(ovarian.dataset, aes(x = area, y = concavity, 
                            color = as.factor(diagnosis))) +
  geom_point(size = 1) +
  labs(x = "Area", y = "Concavity", color = "Diagnosis") +
  scale_color_manual(values = c("springgreen", "hotpink"))
```

Q1.5. What is the difference between the two plots? Which one gives you better separation between the classes and why?

The PCA plot from question 1.3 differs from the plot in 1.4 in several ways. For one, the ggplot graph maps only the area vs the concavity, while the PCA plot graphs both those features along with the rest of the data. The ggplot graph is limited in that sense, because it can only display two features within a 2D grid, while the PCA graph is not bound by those constraints. Due to this, the PCA graph is better than the ggplot graph as it can help illustrate better class separation for a multitude of classes verses the ggplot's 2. 

Q1.6. (bonus): Plot the distribution of the PCs. Hint: you can use boxplot on the transformed dataset. 
```{r}
boxplot(ovarian.pca$x[,c(1:30)], xlab = "Principle Components", ylab = "Value", 
        las = 2, cex.axis = 0.8)
```
## Question 2 Clustering

Q2.1. Apply kmeans clustering on the data and identify two clusters within your dataset. What is the concordance between the clusters that you have identified and the true labels of the cells (Benign vs Malignant).
```{r}
library(dplyr)

#scale the data
ovarian_scaled <- scale(ovarian.dataset[, c(1,3:32)])

# Apply k-means 
set.seed(786) 
kmeans_result <- kmeans(ovarian_scaled, centers = 2, nstart = 25)

# Map clusters to diagnosis labels 
predicted_labels <- ifelse(kmeans_result$cluster == 1, "M", "B")

table(Cluster = predicted_labels, ovarian.dataset$diagnosis)
```

Q2.2. Repeat the kmeans analysis 10 times and report the mean accuracy across the 10 runs. Why are the results different in each run? 

  The mean accuracy across 10 runs: 0.9216
  
  A k-means clustering analysis starts with randomly chosen centroids, which can lead to different final clusters due to varying initial points. This randomness causes variations in clustering outcomes and thus leads to different accuracies for each run.
  
```{r}
accuracies <- numeric(10)

for (i in 1:10) {
  # Run k-means with 2 clusters
  kmeans_result <- kmeans(ovarian_scaled, centers = 2, nstart = 25)
  predicted_labels <- ifelse(kmeans_result$cluster == 1, "M", "B")
  
  # Calculate accuracy by comparing predicted labels to actual labels
  correct_labels <- ifelse(predicted_labels == ovarian.dataset$diagnosis, 1, 0)
  
  # Calculate accuracy
  accuracy <- sum(correct_labels) / length(correct_labels)
  
  reverse_accuracy <- sum(1 - correct_labels) / length(correct_labels)
  accuracies[i] <- max(accuracy, reverse_accuracy)
  print(reverse_accuracy)
}

# Calculate mean accuracy across 10 runs
mean(accuracies)

```
Q2.3. Repeat the same analysis but with the top 5 PCs. 

  The mean accuracy across 10 runs with the top 5 PCs: 0.9184

  The different values in this test are due to the same reason as Q2.2. 
```{r}
accuracies_5PCs <- numeric(10)

for (i in 1:10) {
  # Run k-means with 2 clusters
  set.seed(123)
  kmeans_result <- kmeans(ovarian.pca$x[, 1:5], centers = 2, nstart = 25)
  predicted_labels <- ifelse(kmeans_result$cluster == 1, "M", "B")
  
  # Calculate accuracy by comparing predicted labels to actual labels
  correct_labels <- ifelse(predicted_labels == ovarian.dataset$diagnosis, 1, 0)
  
  # Calculate accuracy
  accuracy <- sum(correct_labels) / length(correct_labels)
  
  reverse_accuracy <- sum(1 - correct_labels) / length(correct_labels)
  accuracies_5PCs[i] <- max(accuracy, reverse_accuracy)
}

# Calculate mean accuracy across 10 runs 
mean(accuracies_5PCs)

```
Q2.4. Compare the results between Q2.2. and Q2.3. 

  The mean accuracy is slightly better in Q2.2, this suggests that the additional principle components beyond first 5 contain some information that helps in distinguishing between the clusters.

## Question 3 Classification

Q3.1. Design a logistic regression classifier to identify (differentiate) benign and malignant cells. Report the performance of the classification technique on the training and test sets. You can report accuracy, precision and recall. Compare the performance of the classifier on the training and test set and provide a reason as to why one is better than the other. 
```{r}
# Convert diagnosis to binary values (1 for Malignant, 0 for Benign)
ovarian.dataset$diagnosis_binary <- ifelse(ovarian.dataset$diagnosis == "M", 1, 0)

# Split the dataset into training and test sets
ovarian.dataset.train <- ovarian.dataset[sample(nrow(ovarian.dataset))[1:(nrow(ovarian.dataset)/2)],]
ovarian.dataset.test <- ovarian.dataset[sample(nrow(ovarian.dataset))[(nrow(ovarian.dataset)/2):(nrow(ovarian.dataset))],]

# Train the logistic regression model on the training set using the binary diagnosis variable
logistic_model <- glm(diagnosis_binary ~ perimeter + area + smoothness + symmetry + concavity + 
                      protein1 + protein2 + protein3 + protein4 + protein5,
                      data = ovarian.dataset.train, family = binomial)

# Make predictions on the training set
train_pred <- predict(logistic_model, ovarian.dataset.train, type = "response")
train_class <- ifelse(train_pred >= 0.5, "M", "B")

# Make predictions on the test set
test_pred <- predict(logistic_model, ovarian.dataset.test, type = "response")
test_class <- ifelse(test_pred >= 0.5, "M", "B")

# Function to calculate accuracy, precision, and recall
evaluate_performance <- function(true_labels, predictions) {
  tp <- sum(predictions == "M" & true_labels == "M")
  tn <- sum(predictions == "B" & true_labels == "B")
  fp <- sum(predictions == "M" & true_labels == "B")
  fn <- sum(predictions == "B" & true_labels == "M")
  
  accuracy <- (tp + tn) / length(true_labels)
  precision <- tp / (tp + fp)
  recall <- tp / (tp + fn)
  
  return(c(accuracy = accuracy, precision = precision, recall = recall))
}

# Evaluate model performance as before
train_performance <- evaluate_performance(ovarian.dataset.train$diagnosis, train_class)
print("Training Performance")
print(train_performance)

test_performance <- evaluate_performance(ovarian.dataset.test$diagnosis, test_class)
print("Test Performance")
print(test_performance)

```
  The test sets show a better value in accuracy, precision, and recall. This may be due to the fact that logistic regression captures generalisable patterns that align more closely with the broader data distribution than seen in the test set.

Q3.2. Repeat the same task as Q3.1. with the top 5 PCs.
```{r}
# Prepare the data by using the first 5 PCs
ovarian.dataset$PC1 <- ovarian.pca$x[, 1]
ovarian.dataset$PC2 <- ovarian.pca$x[, 2]
ovarian.dataset$PC3 <- ovarian.pca$x[, 3]
ovarian.dataset$PC4 <- ovarian.pca$x[, 4]
ovarian.dataset$PC5 <- ovarian.pca$x[, 5]

# Convert diagnosis to binary values (1 for Malignant, 0 for Benign)
ovarian.dataset$diagnosis_binary <- ifelse(ovarian.dataset$diagnosis == "M", 1, 0)

# Split the dataset into training and test sets
ovarian.dataset.train <- ovarian.dataset[sample(nrow(ovarian.dataset))[1:(nrow(ovarian.dataset)/2)],]
ovarian.dataset.test <- ovarian.dataset[sample(nrow(ovarian.dataset))[(nrow(ovarian.dataset)/2):(nrow(ovarian.dataset))],]

# Train the logistic regression model using the top 5 PCs
logistic_model_pcs <- glm(diagnosis_binary ~ PC1 + PC2 + PC3 + PC4 + PC5,
                          data = ovarian.dataset.train, family = binomial)

# Make predictions on the training set
train_pred_pcs <- predict(logistic_model_pcs, ovarian.dataset.train, type = "response")
train_class_pcs <- ifelse(train_pred_pcs >= 0.5, "M", "B")

# Make predictions on the test set
test_pred_pcs <- predict(logistic_model_pcs, ovarian.dataset.test, type = "response")
test_class_pcs <- ifelse(test_pred_pcs >= 0.5, "M", "B")

# Evaluate model performance as before
train_performance_pcs <- evaluate_performance(ovarian.dataset.train$diagnosis, train_class_pcs)
print("Training Performance with Top 5 PCs")
print(train_performance_pcs)

test_performance_pcs <- evaluate_performance(ovarian.dataset.test$diagnosis, test_class_pcs)
print("Test Performance with Top 5 PCs")
print(test_performance_pcs)

```
Q3.3. Compare the results between Q3.1. and Q3.2. Do the results get better or worse? Why?

Between 3.1 and 3.2, Q3.2 gives a higher amount of accuracy and precision along with a higher recall rate.

The formula of accuracy is as follows: (True + False Positives)/Total Observations 
The formula of precision is as follows: (True Positives)/(True Positives + False Positives)
The formula of recall is as follows: (True Positives)/(True Positives + False Negatives)

As Q3.2's result is higher in all three cases, it implies that it has a high level of true positives and a low degree of false positives and false negatives compared to Q3.1. 

When PCA testing is done, it reduces the dimensions of the data, thereby removing all the redundant noise and "extra" data in the set, only visualising the most important variability. As this extra data is removed, it allows only the most important of information to remain in the dataset, and therefore increasing both the accuracy and precision as redundant data is not there to "stain" the dataset. 

By only including the top five PCs, the five most important PCs with the most variability, Q3.2 calculates its result with the most important data, which further leads to more accurate and precise results with high recall. 

However, it is important to note that Q3.1's result is also quite high, so it is not a terrible representation of the data, it is just that Q3.2's method gives a more better answer. 

Q3.4. Compare the results of the clustering and classification methods. Which one gives you better result?

Compared to the clustering method from Q2, it can be seen that the classification method from Q3 provides a more accurate result. The accuracy from the clustering method averaged to 0.9216 for all data during 10 runs, contrasted by Q3's accuracy of 0.9424920. For the 5 PC dataset, the clustering method gave a result with 0.9184 accuracy, lower than that of its prior test with more data, while the classification accuracy went up to a value of 0.9744409. 

Classification is a supervised machine learning method while clustering is unsupervised. As supervised methods are those that are given more clear instructions and labels, it learns from the provided information which can in turn give a more accurate result than unsupervised methods that have to do all the heavy lifting themselves. Clustering, as it is unsupervised, may make data groups that are not the the intended representations. As classification is bound by the limits imposed upon it, it has less leeway to make error. 

While both methods gave results with a high accuracy of greater than 90%, it is easy to note the fact that the classification method gives a decently more accurate result.  

Q3.5. Install the library "ROCR" and load it. Then, run the following lines using your trained logistic regression model:

  The ROC curve indicates a strong class separability, with minimal overlap between benign and malignant cells, as evidenced by its proximity to the top-left corner. This suggests that the model effectively distinguishes between the classes. Unlike a single sensitivity or specificity measure, the ROC curve offers a comprehensive view of performance across thresholds, helping to visualize trade-offs between true positives and false positives.
```{r}
#install.packages("ROCR")
library(ROCR)

pred.prob <- predict(logistic_model, ovarian.dataset, type="response")
predict <- prediction(pred.prob, ovarian.dataset$diagnosis, label.ordering=c("B","M"))
perform <- performance(predict,"tpr","fpr")
plot(perform,colorize=TRUE)
```

Q3.6 . Design another classifier (using a different classification method) and repeat Q3.1-3.

```{r}
#install.packages("randomForest")
library(randomForest)
```


```{r}
# Convert diagnosis to binary values (1 for Malignant, 0 for Benign)
ovarian.dataset$diagnosis_binary <- ifelse(ovarian.dataset$diagnosis == "M", 1, 0)
# Split the dataset into training and test sets
ovarian.dataset.train <- ovarian.dataset[sample(nrow(ovarian.dataset))[1:(nrow(ovarian.dataset)/2)],]
ovarian.dataset.test <- ovarian.dataset[sample(nrow(ovarian.dataset))[(nrow(ovarian.dataset)/2):(nrow(ovarian.dataset))],]
# Train the logistic regression model on the training set using the binary diagnosis variable
randomForest_model <- randomForest(diagnosis_binary ~ perimeter + area + smoothness + symmetry + concavity +
protein1 + protein2 + protein3 + protein4 + protein5,
data = ovarian.dataset.train)
# Make predictions on the training set
train_pred <- predict(logistic_model, ovarian.dataset.train, type = "response")
train_class <- ifelse(train_pred >= 0.5, "M", "B")
# Make predictions on the test set
test_pred <- predict(logistic_model, ovarian.dataset.test, type = "response")
test_class <- ifelse(test_pred >= 0.5, "M", "B")
# Function to calculate accuracy, precision, and recall
evaluate_performance <- function(true_labels, predictions) {
tp <- sum(predictions == "M" & true_labels == "M")
tn <- sum(predictions == "B" & true_labels == "B")
fp <- sum(predictions == "M" & true_labels == "B")
fn <- sum(predictions == "B" & true_labels == "M")
accuracy <- (tp + tn) / length(true_labels)
precision <- tp / (tp + fp)
recall <- tp / (tp + fn)
return(c(accuracy = accuracy, precision = precision, recall = recall))
}
# Evaluate model performance as before
train_performance <- evaluate_performance(ovarian.dataset.train$diagnosis, train_class)
print("Training Performance")
```
```{r}
print(train_performance)
```

```{r}
test_performance <- evaluate_performance(ovarian.dataset.test$diagnosis, test_class)
print("Test Performance")
print(test_performance)
```

```{r}
# Prepare the data by using the first 5 PCs
ovarian.dataset$PC1 <- ovarian.pca$x[, 1]
ovarian.dataset$PC2 <- ovarian.pca$x[, 2]
ovarian.dataset$PC3 <- ovarian.pca$x[, 3]
ovarian.dataset$PC4 <- ovarian.pca$x[, 4]
ovarian.dataset$PC5 <- ovarian.pca$x[, 5]
# Convert diagnosis to binary values (1 for Malignant, 0 for Benign)
ovarian.dataset$diagnosis_binary <- ifelse(ovarian.dataset$diagnosis == "M", 1, 0)
# Split the dataset into training and test sets
ovarian.dataset.train <- ovarian.dataset[sample(nrow(ovarian.dataset))[1:(nrow(ovarian.dataset)/2)],]
ovarian.dataset.test <- ovarian.dataset[sample(nrow(ovarian.dataset))[(nrow(ovarian.dataset)/2):(nrow(ovarian.dataset))],]
# Train the logistic regression model using the top 5 PCs
randomForest_model_pcs <- randomForest(diagnosis_binary ~ PC1 + PC2 + PC3 + PC4 + PC5,
data = ovarian.dataset.train)
## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
# Make predictions on the training set
train_pred_pcs <- predict(logistic_model_pcs, ovarian.dataset.train, type = "response")
train_class_pcs <- ifelse(train_pred_pcs >= 0.5, "M", "B")
# Make predictions on the test set
test_pred_pcs <- predict(logistic_model_pcs, ovarian.dataset.test, type = "response")
test_class_pcs <- ifelse(test_pred_pcs >= 0.5, "M", "B")
# Evaluate model performance as before
train_performance_pcs <- evaluate_performance(ovarian.dataset.train$diagnosis, train_class_pcs)
print("Training Performance with Top 5 PCs")
print(train_performance_pcs)

```



```{r}
test_performance_pcs <- evaluate_performance(ovarian.dataset.test$diagnosis, test_class_pcs)
print("Test Performance with Top 5 PCs")
print(test_performance_pcs)
```
 
When creating a design for another classifier, we used the randomForest classification method. 

When running the data through this particular test, we found the data to have a precision of 100%, meaning that the number of false positives was equal to zero. This could either mean that the data training set never included any false positives or that randomForest was unable to find any false positives within the dataset. 

We ran the test again, and found new values: 
For accuracy,  we found the accuracy to be 0.3% higher than the linear regression method, but this time, the precision was lower than the first method by about 0.8%. 

When comparing this method of classification to the linear regression model we used previously, we find that both give us similar outputs, meaning that the difference between the two methods is with the algorithm itself and these differences don't greatly change the outcome. Some of these differences include: the processing power used, the number of iterations, and the efficiency of the algorithm. Either way, both of these methods are viable options for classifying the data and identifying whether we get true/false positives and true/false negatives since they both yield high 90 percent on accuracy, precision and recall.

When we focus the randomForest test model onto 5 PCs, rather than the entire dataset, we further remove all the redundant noise and are left with higher values in accuracy, precision, and recall. This is similar to what we see in the linear regression model, however, within the randomForest method, we see a higher increase in recall by 7% verses the linear regression's 3%.


Collaboration Statement
All team members contributed to the code and proofreading.
