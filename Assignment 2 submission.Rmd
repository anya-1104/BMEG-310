---
title: "Assignment 2 submission"
author: "Anya Wongreantong, Willem Barnard, "
date: "2024-10-16"
output:
  pdf_document: default
  html_document: default
---
Perform exploratory analysis of the measured features and see if we can design machine learning tools to separate the benign (B) and malignant (M) cells.
```{r}
#load the data

ovarian.dataset <- read.delim("ovarian.data", sep=",", header = FALSE)
features <- c("perimeter", "area", "smoothness", "symmetry", "concavity", paste("protein", seq(1, 25), sep=""))
names(ovarian.dataset) <- c("cell_id", "diagnosis", features) 

head(ovarian.dataset)
```
## Question 1 dimensionality reduction

Q1.1. Perform PCA on the features of the dataset. How much of the variation in the data is associated with PC1?

  proportion of variance explained by PC1: 0.4277
```{r}
ovarian.pca <- prcomp(ovarian.dataset[ , features], center = TRUE, scale. = TRUE)
summary(ovarian.pca)
```
Q1.2. You want to represent 90% of the variance in the data by dimensionality reduction. How many PCs do you need to achieve this? In other word, what would be the dimensionality of the reduced feature space so that you preserve 90% of the variability in the data? 

  The cumulative proportion for PC9 is 0.90101. Therefore, 9PCs are needed to represent at least 90% of the variance in the data

Q1.3. As you should know by now, PCA transforms the data into a new space. In a 2-D plot, can you plot the observations corresponding to the first two important PCs? Note, use two different colors to represent the two classes of cells.

```{r}
# install.packages("devtools")
library(devtools)
# install.packages("remotes")
remotes::install_github("vqv/ggbiplot")
library(ggbiplot)

ggbiplot(ovarian.pca, choices = c(1,2), obs.scale = 1, var.scale = 1,
         groups = ovarian.dataset$diagnosis, ellipse = TRUE, circle = TRUE) + 
  scale_color_manual(values = c("royalblue", "plum")) 
```

Q1.4. Can you plot the "area" and "concavity"  features associated with the cells?
```{r}
library(ggplot2)

ggplot(ovarian.dataset, aes(x = area, y = concavity, 
                            color = as.factor(diagnosis))) +
  geom_point(size = 1) +
  labs(x = "Area", y = "Concavity", color = "Diagnosis") +
  scale_color_manual(values = c("springgreen", "hotpink"))
```

Q1.5. What is the difference between the two plots? Which one gives you better separation between the classes and why?

  PCA plot provides better separation between classes because it maximizes variance across multiple features. While area vs concavity plot is limited to only 2 original features. PCA is more effective for visualizing class differences for multiple features in class separation.

Q1.6. (bonus): Plot the distribution of the PCs. Hint: you can use boxplot on the transformed dataset. 
```{r}
boxplot(ovarian.pca$x[,c(1:30)], xlab = "Principle Components", ylab = "Value", 
        las = 2, cex.axis = 0.8)
```
## Question 2 Clustering

Q2.1. Apply kmeans clustering on the data and identify two clusters within your dataset. What is the concordance between the clusters that you have identified and the true labels of the cells (Benign vs Malignant).
```{r}
library(dplyr)

#scale the data
ovarian_scaled <- scale(ovarian.dataset[, c(1,3:32)])

# Apply k-means 
set.seed(786) 
kmeans_result <- kmeans(ovarian_scaled, centers = 2, nstart = 25)

# Map clusters to diagnosis labels 
predicted_labels <- ifelse(kmeans_result$cluster == 1, "M", "B")

table(Cluster = predicted_labels, ovarian.dataset$diagnosis)
```

Q2.2. Repeat the kmeans analysis 10 times and report the mean accuracy across the 10 runs. Why are the results different in each run? 

  mean accuracy across 10 runs: 0.9216
  K-means clustering starts with randomly chosen centroids, which can lead to different final clusters due to varying initial points. This randomness causes variations in clustering outcomes and thus different accuracies for each run.
```{r}
accuracies <- numeric(10)

for (i in 1:10) {
  # Run k-means with 2 clusters
  kmeans_result <- kmeans(ovarian_scaled, centers = 2, nstart = 25)
  predicted_labels <- ifelse(kmeans_result$cluster == 1, "M", "B")
  
  # Calculate accuracy by comparing predicted labels to actual labels
  correct_labels <- ifelse(predicted_labels == ovarian.dataset$diagnosis, 1, 0)
  
  # Calculate accuracy
  accuracy <- sum(correct_labels) / length(correct_labels)
  
  reverse_accuracy <- sum(1 - correct_labels) / length(correct_labels)
  accuracies[i] <- max(accuracy, reverse_accuracy)
  print(reverse_accuracy)
}

# Calculate mean accuracy across 10 runs
mean(accuracies)

```
Q2.3. Repeat the same analysis but with the top 5 PCs. 

  mean accuracy across 10 runs with the top 5 PCs: 0.9184
```{r}
accuracies_5PCs <- numeric(10)

for (i in 1:10) {
  # Run k-means with 2 clusters
  set.seed(123)
  kmeans_result <- kmeans(ovarian.pca$x[, 1:5], centers = 2, nstart = 25)
  predicted_labels <- ifelse(kmeans_result$cluster == 1, "M", "B")
  
  # Calculate accuracy by comparing predicted labels to actual labels
  correct_labels <- ifelse(predicted_labels == ovarian.dataset$diagnosis, 1, 0)
  
  # Calculate accuracy
  accuracy <- sum(correct_labels) / length(correct_labels)
  
  reverse_accuracy <- sum(1 - correct_labels) / length(correct_labels)
  accuracies_5PCs[i] <- max(accuracy, reverse_accuracy)
}

# Calculate mean accuracy across 10 runs 
mean(accuracies_5PCs)

```
Q2.4. Compare the results between Q2.2. and Q2.3. 

  The mean accuracy is slightly better in Q2.2, this suggests that the addtional principle conponents beyond first 5 contain some information that helps in distinguishing between the clusters.

## Question 3 Classification

Q3.1. Design a logistic regression classifier to identify (differentiate) benign and malignant cells. Report the performance of the classification technique on the training and test sets. You can report accuracy, precision and recall. Compare the performance of the classifier on the training and test set and provide a reason as to why one is better than the other. 
```{r}
# Convert diagnosis to binary values (1 for Malignant, 0 for Benign)
ovarian.dataset$diagnosis_binary <- ifelse(ovarian.dataset$diagnosis == "M", 1, 0)

# Split the dataset into training and test sets
ovarian.dataset.train <- ovarian.dataset[sample(nrow(ovarian.dataset))[1:(nrow(ovarian.dataset)/2)],]
ovarian.dataset.test <- ovarian.dataset[sample(nrow(ovarian.dataset))[(nrow(ovarian.dataset)/2):(nrow(ovarian.dataset))],]

# Train the logistic regression model on the training set using the binary diagnosis variable
logistic_model <- glm(diagnosis_binary ~ perimeter + area + smoothness + symmetry + concavity + 
                      protein1 + protein2 + protein3 + protein4 + protein5,
                      data = ovarian.dataset.train, family = binomial)

# Make predictions on the training set
train_pred <- predict(logistic_model, ovarian.dataset.train, type = "response")
train_class <- ifelse(train_pred >= 0.5, "M", "B")

# Make predictions on the test set
test_pred <- predict(logistic_model, ovarian.dataset.test, type = "response")
test_class <- ifelse(test_pred >= 0.5, "M", "B")

# Function to calculate accuracy, precision, and recall
evaluate_performance <- function(true_labels, predictions) {
  tp <- sum(predictions == "M" & true_labels == "M")
  tn <- sum(predictions == "B" & true_labels == "B")
  fp <- sum(predictions == "M" & true_labels == "B")
  fn <- sum(predictions == "B" & true_labels == "M")
  
  accuracy <- (tp + tn) / length(true_labels)
  precision <- tp / (tp + fp)
  recall <- tp / (tp + fn)
  
  return(c(accuracy = accuracy, precision = precision, recall = recall))
}

# Evaluate model performance as before
train_performance <- evaluate_performance(ovarian.dataset.train$diagnosis, train_class)
print("Training Performance")
print(train_performance)

test_performance <- evaluate_performance(ovarian.dataset.test$diagnosis, test_class)
print("Test Performance")
print(test_performance)

```
  Test sets shows a better value in accuracy, precision, and recall. This may be due to logistic regression captures generalizable patterns that align more closely with the broader data distribution seen in the test set.

Q3.2. Repeat the same task as Q3.1. with the top 5 PCs.
```{r}
# Prepare the data by using the first 5 PCs
ovarian.dataset$PC1 <- ovarian.pca$x[, 1]
ovarian.dataset$PC2 <- ovarian.pca$x[, 2]
ovarian.dataset$PC3 <- ovarian.pca$x[, 3]
ovarian.dataset$PC4 <- ovarian.pca$x[, 4]
ovarian.dataset$PC5 <- ovarian.pca$x[, 5]

# Convert diagnosis to binary values (1 for Malignant, 0 for Benign)
ovarian.dataset$diagnosis_binary <- ifelse(ovarian.dataset$diagnosis == "M", 1, 0)

# Split the dataset into training and test sets
ovarian.dataset.train <- ovarian.dataset[sample(nrow(ovarian.dataset))[1:(nrow(ovarian.dataset)/2)],]
ovarian.dataset.test <- ovarian.dataset[sample(nrow(ovarian.dataset))[(nrow(ovarian.dataset)/2):(nrow(ovarian.dataset))],]

# Train the logistic regression model using the top 5 PCs
logistic_model_pcs <- glm(diagnosis_binary ~ PC1 + PC2 + PC3 + PC4 + PC5,
                          data = ovarian.dataset.train, family = binomial)

# Make predictions on the training set
train_pred_pcs <- predict(logistic_model_pcs, ovarian.dataset.train, type = "response")
train_class_pcs <- ifelse(train_pred_pcs >= 0.5, "M", "B")

# Make predictions on the test set
test_pred_pcs <- predict(logistic_model_pcs, ovarian.dataset.test, type = "response")
test_class_pcs <- ifelse(test_pred_pcs >= 0.5, "M", "B")

# Evaluate model performance as before
train_performance_pcs <- evaluate_performance(ovarian.dataset.train$diagnosis, train_class_pcs)
print("Training Performance with Top 5 PCs")
print(train_performance_pcs)

test_performance_pcs <- evaluate_performance(ovarian.dataset.test$diagnosis, test_class_pcs)
print("Test Performance with Top 5 PCs")
print(test_performance_pcs)

```
  test set shows a slightly better value in accuracy and recall, and slight decrease in precision. The slightly better performance on the test set suggests that the logistic regression model generalizes well and effectively captures the core patterns in the data without overfitting.

Q3.3. Compare the results between Q3.1. and Q3.2. Do the results get better or worse? Why?

  accuracy, precision, and recall improved when using the top 5 PCs in 3.2. The principle Components capture the most significant variance in the data, potentially removing noise and redundant information from the original features. By focusing on the top 5 PCs, the classifier may be better at generalizing, thus leading to better performance on unseen data.

Q3.4. Compare the results of the clustering and classification methods. Which one gives you better result?

  Clustering achieved a mean accuracy of around 0.9216 with 10PCs and 0.9184 with 5 PCs
  Classification had a test accuracy of 0.9585, and using the top 5 PCs improved accuracy further to 0.9776
  Classification provides better results because it leverage labeled data to directly optimize the separation between classes. 

Q3.5. Install the library "ROCR" and load it. Then, run the following lines using your trained logistic regression model:

  The ROC curve indicates strong class separability, with minimal overlap between benign and malignant cells, as evidenced by its proximity to the top-left corner. This suggests that the model effectively distinguishes between classes. Unlike a single sensitivity or specificity measure, the ROC curve offers a comprehensive view of performance across thresholds, helping to visualize trade-offs between true positives and false positives.
```{r}
#install.packages("ROCR")
library(ROCR)

pred.prob <- predict(logistic_model, ovarian.dataset, type="response")
predict <- prediction(pred.prob, ovarian.dataset$diagnosis, label.ordering=c("B","M"))
perform <- performance(predict,"tpr","fpr")
plot(perform,colorize=TRUE)
```

Q3.6 . Design another classifier (using a different classification method) and repeat Q3.1-3.

```{r}
#install.packages("randomForest")
library(randomForest)
```


```{r}
# Convert diagnosis to binary values (1 for Malignant, 0 for Benign)
ovarian.dataset$diagnosis_binary <- ifelse(ovarian.dataset$diagnosis == "M", 1, 0)
# Split the dataset into training and test sets
ovarian.dataset.train <- ovarian.dataset[sample(nrow(ovarian.dataset))[1:(nrow(ovarian.dataset)/2)],]
ovarian.dataset.test <- ovarian.dataset[sample(nrow(ovarian.dataset))[(nrow(ovarian.dataset)/2):(nrow(ovarian.dataset))],]
# Train the logistic regression model on the training set using the binary diagnosis variable
randomForest_model <- randomForest(diagnosis_binary ~ perimeter + area + smoothness + symmetry + concavity +
protein1 + protein2 + protein3 + protein4 + protein5,
data = ovarian.dataset.train)
7
# Make predictions on the training set
train_pred <- predict(logistic_model, ovarian.dataset.train, type = "response")
train_class <- ifelse(train_pred >= 0.5, "M", "B")
# Make predictions on the test set
test_pred <- predict(logistic_model, ovarian.dataset.test, type = "response")
test_class <- ifelse(test_pred >= 0.5, "M", "B")
# Function to calculate accuracy, precision, and recall
evaluate_performance <- function(true_labels, predictions) {
tp <- sum(predictions == "M" & true_labels == "M")
tn <- sum(predictions == "B" & true_labels == "B")
fp <- sum(predictions == "M" & true_labels == "B")
fn <- sum(predictions == "B" & true_labels == "M")
accuracy <- (tp + tn) / length(true_labels)
precision <- tp / (tp + fp)
recall <- tp / (tp + fn)
return(c(accuracy = accuracy, precision = precision, recall = recall))
}
# Evaluate model performance as before
train_performance <- evaluate_performance(ovarian.dataset.train$diagnosis, train_class)
print("Training Performance")
```
```{r}
print(train_performance)
```

```{r}
test_performance <- evaluate_performance(ovarian.dataset.test$diagnosis, test_class)
print("Test Performance")
```

```{r}
print(test_performance)
```
```{r}
# Prepare the data by using the first 5 PCs
ovarian.dataset$PC1 <- ovarian.pca$x[, 1]
ovarian.dataset$PC2 <- ovarian.pca$x[, 2]
ovarian.dataset$PC3 <- ovarian.pca$x[, 3]
ovarian.dataset$PC4 <- ovarian.pca$x[, 4]
8
ovarian.dataset$PC5 <- ovarian.pca$x[, 5]
# Convert diagnosis to binary values (1 for Malignant, 0 for Benign)
ovarian.dataset$diagnosis_binary <- ifelse(ovarian.dataset$diagnosis == "M", 1, 0)
# Split the dataset into training and test sets
ovarian.dataset.train <- ovarian.dataset[sample(nrow(ovarian.dataset))[1:(nrow(ovarian.dataset)/2)],]
ovarian.dataset.test <- ovarian.dataset[sample(nrow(ovarian.dataset))[(nrow(ovarian.dataset)/2):(nrow(ovarian.dataset))],]
# Train the logistic regression model using the top 5 PCs
randomForest_model_pcs <- randomForest(diagnosis_binary ~ PC1 + PC2 + PC3 + PC4 + PC5,
data = ovarian.dataset.train)
## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
# Make predictions on the training set
train_pred_pcs <- predict(logistic_model_pcs, ovarian.dataset.train, type = "response")
train_class_pcs <- ifelse(train_pred_pcs >= 0.5, "M", "B")
# Make predictions on the test set
test_pred_pcs <- predict(logistic_model_pcs, ovarian.dataset.test, type = "response")
test_class_pcs <- ifelse(test_pred_pcs >= 0.5, "M", "B")
# Evaluate model performance as before
train_performance_pcs <- evaluate_performance(ovarian.dataset.train$diagnosis, train_class_pcs)
print("Training Performance with Top 5 PCs")


```

```{r}
print(train_performance_pcs)
```

```{r}
test_performance_pcs <- evaluate_performance(ovarian.dataset.test$diagnosis, test_class_pcs)
print("Test Performance with Top 5 PCs")
```

```{r}
print(test_performance_pcs)
```
 
In this case we used the randomForest classification method which we found when running the training data to have a percision of 100% which means that the number of false positives is equal to zero. This means the training set of data never included any false positive or that is could not find any false positives. We also found the outcome of the two methods to be very similar but the accuracy using the randomForest method was higher by0.3 percent and the precision was lower by 0.8 percent.
